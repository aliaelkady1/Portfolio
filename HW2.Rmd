---
title: "HW2"
date: "2024-01-22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kknn)
library(caret)
library(ggplot2)
rm(list = ls())
data = read.table("C:/Users/aliae/Desktop/ISYE_HW/HW2/credit_card_data-headers.txt", header = TRUE)
set.seed(41)

```
### ISYE Homework 2

Question 3.1 (a)      
Cross validation is a method of comparing models that utilizes splitting the data into two parts. One of the parts get split further into several folds (k) and the models get trained on n-1 folds, then validated on the nth fold.
After obtaining the accuracy for each model, we then pick the final model by testing it on the second part of the data set (which it wasn't trained on previously).

I split the data likewise; 80% training/validation and 20% testing. I did this by first obtaining a random sample of indices (createDataPartition()) containing 80% of the original data set, then building the data matrix with these corresponding indices. A second data matrix was created from the remaining 20% of the indices. The training set would be used for training/validation and the testing set would be used only after comparing the models and picking the best one.

How I plan to tackle this question is by building a function that will take in as input the number of k-neighbors and build a corresponding model utilizing cross validation then returning the accuracy for said model. Then I will compare the accuracy for each model, and the model with the highest accuracy will then be chosen and tested with the testing data.
A k-value corresponding to 6 had the highest accuracy (=0.8568702) and after it came k=11 with an accuracy of (=0.8549618), so a model was built for each and tested with the testing data. Both models got an accuracy of (0.8461538). This is understandable since the model is being tested on data it hasn't "seen" before therefore it is logical that the accuracy will be a bit less than that of the training data.

```{r}
###Cross validation###

#80/20 data split
trainIndices = createDataPartition(data$R1,p=.80, list = FALSE) #indices of samples data
trainingData = data[trainIndices,] #actual training set
testData = data[-trainIndices,] #remaining 20% as test data


kcvModelBuilder = function(x){
  kModel <- cv.kknn(formula = R1~., trainingData, kcv = 10, k=x, scale = TRUE)
  rounded = as.integer( (kModel[[1]][,2]) +0.5)
  accuracy = sum(rounded == trainingData[,11]) / nrow(trainingData)
  return(accuracy)
}

#comparing models

for(i in 1:20){
  kcvModelAccuracy = kcvModelBuilder(i)
  cat("With a k(neighbors) value of ",i," the accuracy was ",kcvModelAccuracy, sep="", fill = TRUE)
}

#test data with model k=6 and k= 11 given that it performed the best with these two k values
kModelCvFinal6 = kknn(as.factor(R1)~. , train = trainingData, test = testData, k=6, scale = TRUE)

finalKModelAccuracy6 = sum(fitted(kModelCvFinal6)== testData[,11] ) / nrow(testData)
finalKModelAccuracy6


kModelCvFinal11 = kknn(as.factor(R1)~. , train = trainingData, test = testData, k=11, scale = TRUE)

finalKModelAccuracy11 = sum(fitted(kModelCvFinal11)== testData[,11] ) / nrow(testData)
finalKModelAccuracy11

```
Question 3.1(b)     
Instead of using cross validation, here we will use a simpler approach where the data will initially be split into training, validation, and testing parts. Models are trained using the training data and then validated using the validation data. The model(s) that performed the best with the validation data will be chosen and tested on the testing data.

Following a similar approach as question 3.1(a), I first split the data using a random sample of indices that corresponds to 60% of the data. This will be the training set. Then the remaining 40% of the data set gets divided into two parts; validation and testing.

As before, I created a function that will build the kknn models but this time not using cross validation. The function will return the accuracy. The models will be trained with the training data (60%) and tested using the validation data partition (20%).

Since k=12 and k=13 had the highest accuracy rate, a final model was built with the corresponding k-values and tested using the testing data partition (20%).Both models yielded the same final accuracy value. 

```{r}
###splitting the data

splitTrainingIndices = createDataPartition(data$R1,p=.60, list = FALSE) #indices of training data

splitTrainData = data[splitTrainingIndices,] #training set

remainingData = data[-splitTrainingIndices,] #remaining 40% to be further split for test data
splitTestIndices = createDataPartition(remainingData$R1,p=.50,list=FALSE)

splitTestData = remainingData[splitTestIndices,] #test data
splitValidData = remainingData[-splitTestIndices,] #validation data

kModelBuiler = function(x){
  kModelTemp = kknn(as.factor(R1)~. , train = splitTrainData, test = splitValidData, k=x, scale = TRUE)
  kModelTempAccuracy = sum(fitted(kModelTemp)== splitValidData[,11] ) / nrow(splitValidData)
  return(kModelTempAccuracy)
}

#comparing models

for(i in 1:20){
  kModelAccuracy = kModelBuiler(i)
  cat("With a k(neighbors) value of ",i," the accuracy was ",kModelAccuracy, sep="", fill = TRUE)
  
}


##test the best models (k=12 & k=13) using the test data

kModel12Final = kknn(as.factor(R1)~. , train = splitTrainData, test = splitTestData, k=12, scale = TRUE)
kModel13Final = kknn(as.factor(R1)~. , train = splitTrainData, test = splitTestData, k=13, scale = TRUE)

sum(fitted(kModel12Final)== splitTestData[,11] ) / nrow(splitTestData)
sum(fitted(kModel13Final)== splitTestData[,11] ) / nrow(splitTestData)

```

Question 4.1:
Coming from a biology background, we are often faced with the immense similarities between loads of animals/species and it can sometimes be not so clear whether two animals are from the same species or neighboring ones. One way to distinguish would be genetic profiling but that can sometimes be expensive. Therefore, a clustering model could be a really helpful initial indicator. 
My partner did his research on a certain species of spiders, and in order for him to identify the spider visually he had to take note of factors such as web-shape formation, diameter of the abdomen of the spider, color, number of stripes present, length of limbs etc. 
One might be tempted to classify a spider as a certain specie visually, but it might end up being from a very similar looking specie (did you know that different spiders create different looking webs?). A cluster model that uses the above factors and others (eg, weight) as predictors could help distinguish spiders.
I believe a model like this would not only help in spider classification but loads of other animals as this is a field that is ever growing. As we advance in science and tech, we realize that we either have misclassified animals as being other species or we completely ruled out a specie. Extensive genetic testing helped draw lines between very visually similar animals and I believe a clustering model can act as an preliminary identifier. 

```{r include = FALSE}
####clustering####
irisData = read.table("C:/Users/aliae/Desktop/ISYE_HW/HW2/iris.txt", header = TRUE)
```

Question 4.2:      
Visualizing the data set in a scatter plot matrix helped me identify potential predictors to test (each one is written in more details later). The whole purpose of testing different predictors is to see which combination would help make the data points "scatter" a bit to make it easier for our clustering model to cluster correctly.

```{r}
plot(irisData)

```

Out of curiosity, I wanted to see certain predictors in a plot on their own so I tested: 
- petal width x sepal length
- petal width x sepal width 
- sepal width x petal width
- petal width x petal length

This is to get a basic idea about the shape of the data we are working with. I tested more predictors later on. All four graphs seem to scatter the points well but the clearest was the (petal width x petal length) graph. This is just a preliminary observation which would be evaluated by actually building the models.
```{r}
#try different predictors 
dataScaled = scale(irisData[,1:4])
dataScaled <- data.frame(dataScaled, Species=irisData$Species)

ggplot(dataScaled,aes(Petal.Width,Sepal.Length,color=Species))+geom_point()
ggplot(dataScaled,aes(Petal.Width,Sepal.Width,color=Species))+geom_point()
ggplot(dataScaled,aes(Sepal.Width,Petal.Width,color=Species))+geom_point()
ggplot(dataScaled,aes(Petal.Width,Petal.Length,color=Species))+geom_point() 

```

Similarly, I built a function to create the kmeans models each with a different k-value. The function returns the total within-cluster sum of squares, which is basically the total distances of the data points from the their centroid. This gives us an idea about the total variance in the model which would then be compared. The purpose of this part is to put all the returned variances for the different kmeans models in an elbow plot to see how many clusters we should be building a model on.
Based on the plot below, the kink is most obvious at k=3 therefore I'll build the model using 3 clusters.
```{r}
#elbow; xaxis = number of clusters, yaxis= total distances per cluster
#vector to put all the total distance per cluster to be used in the elbow plot

getDistances = function(x){
  #x would be a k 
  tempKMeans <- kmeans(dataScaled[,-5], centers = x, nstart=25,iter.max=30)
  return(tempKMeans$tot.withinss)
}

allDistances = rep(0,20)

for(i in 1:20){ #testing different cluster numbers
  allDistances[i] = getDistances(i)
}

plot(1:20,y=allDistances,type="p")
#based on this plot we can see that we should test for 3 clusters.
```

Now that I know how many clusters I should use, I'll test different predictors. 
As shown by the tables below, the predictors that yielded the best accuracy were
petal length and petal width.

By using these predictors and k (cluster) value of 3, the total number of misclassifications was 6 which is how well the best clustering model did.

```{r}

#now trying different predictors using 3 clusters
#sepal length and petal width
sL.pW = kmeans(dataScaled[,c(1,4)], centers = 3, nstart=25)
table(sL.pW$cluster, dataScaled$Species)

#sepal width and petal width
sW.pW = kmeans(dataScaled[,c(2,4)], centers = 3, nstart=25)
table(sW.pW$cluster, dataScaled$Species)

#sepal width and sepal length
sW.sL = kmeans(dataScaled[,c(1,2)], centers = 3, nstart=25)
table(sW.sL$cluster, dataScaled$Species)

#petal length and petal width
pW.pL = kmeans(dataScaled[,c(3,4)], centers = 3, nstart=25)
table(pW.pL$cluster, dataScaled$Species)

#testing columns (123),(124),(134),(234)
cl.123 = kmeans(dataScaled[,c(1,2,3)], centers = 3, nstart=25)
table(cl.123$cluster, dataScaled$Species)

cl.124 = kmeans(dataScaled[,c(1,2,4)], centers = 3, nstart=25)
table(cl.124$cluster, dataScaled$Species)

cl.134 = kmeans(dataScaled[,c(1,3,4)], centers = 3, nstart=25)
table(cl.134$cluster, dataScaled$Species)

cl.234 = kmeans(dataScaled[,c(2,3,4)], centers = 3, nstart=25)
table(cl.234$cluster, dataScaled$Species)
```

Citations:
- Office hours code
- ISYE Homework 1
- https://shorturl.at/hz356
- https://shorturl.at/nrBLZ


